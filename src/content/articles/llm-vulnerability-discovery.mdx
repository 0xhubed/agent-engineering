---
title: "LLM-Driven Vulnerability Discovery"
description: "How to architect autonomous AI agents that find, validate, and triage security vulnerabilities in real codebases using sandboxed tool access and multi-stage reasoning."
category: "Advanced"
date: "2026-02-21"
type: "topic"
source_url: "https://red.anthropic.com/2026/zero-days/"
source_title: "Evaluating and mitigating the growing risk of LLM-discovered 0-days"
authors: "Nicholas Carlini, Keane Lucas, Evyatar Ben Asher, Newton Cheng, Hasnain Lakhani, David Forsythe, Kyla Guru"
shared_by: "@trq212 on X"
tags:
  - security
  - tool-use
  - agent-patterns
  - evaluation
  - sandboxing
featured: false
---

Large language models are increasingly capable of reasoning about code at a level that lets them discover security vulnerabilities traditional tools miss. Unlike fuzzers, which probe programs with random inputs, LLM-based agents read code the way a human researcher would—tracing logic paths, recognizing historically dangerous patterns, and constructing targeted inputs. Deploying these agents responsibly at scale requires careful attention to sandboxing, validation pipelines, and misuse safeguards.

## The Core Architecture

The canonical setup for autonomous vulnerability discovery places a capable LLM inside an isolated execution environment—typically a virtual machine or container—equipped with standard development utilities and security tooling such as debuggers, address sanitizers, and fuzzers. Crucially, the agent is given no custom harness or task-specific scaffolding. This design choice has a practical purpose: it tests the model's general-purpose reasoning against the actual environment a human researcher would encounter, rather than measuring how well a specialized wrapper performs.

<Diagram>
```
┌─────────────────────────────────────────────────────┐
│                   Orchestration Layer               │
│  (task dispatch, deduplication, prioritization)     │
└───────────────────┬─────────────────────────────────┘
                    │
        ┌───────────▼───────────┐
        │    LLM Agent Core     │
        │  (reasoning + planning│
        └───────────┬───────────┘
                    │  tool calls
        ┌───────────▼───────────────────────────┐
        │         Sandboxed VM / Container       │
        │  ┌─────────┐  ┌──────────┐  ┌──────┐  │
        │  │ Debugger│  │  Fuzzer  │  │ ASAN │  │
        │  └─────────┘  └──────────┘  └──────┘  │
        │         Target Source Code             │
        └────────────────────────────────────────┘
                    │  crash / trace
        ┌───────────▼───────────┐
        │   Validation Pipeline │
        │ (critique → repro →   │
        │  human review)        │
        └───────────────────────┘
```
</Diagram>

The agent can freely decide whether to fuzz, perform static analysis, read commit history, or do any combination of these. In practice, LLM agents often begin with static reasoning—reading recent patches to find similar unfixed patterns—before falling back to dynamic testing. This mirrors how experienced security researchers work and differs fundamentally from purely automated fuzzing pipelines.

## Why LLMs Find Bugs Fuzzers Miss

Fuzz testing is extraordinarily effective at finding shallow bugs exposed by malformed input. However, it struggles with vulnerabilities that require semantically meaningful, internally consistent payloads—inputs that must satisfy multiple constraints at once to reach a vulnerable code path. LLMs can read the surrounding logic and reason about what those constraints are, then construct an input that satisfies them directly.

A concrete example: a file-format parser might only reach a vulnerable code path if a header field correctly encodes the length of a subsequent variable-length structure. A fuzzer will rarely produce the correct relationship by chance across thousands of fields. An LLM reading the parsing code can infer the relationship and synthesize a conformant-but-malicious input on the first attempt.

LLMs are also effective at recognizing vulnerability classes from code patterns. A model trained on large bodies of security research and patch history can recognize that a particular integer arithmetic pattern is structurally similar to historical CVEs, even when the surrounding code is novel.

<Callout type="info">
LLM agents find a complementary set of bugs to fuzzers rather than a strict superset. Deploying both in parallel—and using the LLM to write targeted fuzzing harnesses—yields better coverage than either approach alone.
</Callout>

## Validation Pipelines and False-Positive Control

LLM agents can hallucinate vulnerabilities—reporting bugs that do not actually exist. Hallucinated reports impose real costs on open-source maintainers who must investigate each claim. A multi-stage validation pipeline is essential before any finding leaves the agent environment.

A practical pipeline looks like this:

1. **Crash reproduction**: the agent attempts to reproduce the crash deterministically using address sanitizers (`ASan`, `MSan`, `UBSan`).
2. **Critique pass**: the same or a separate LLM instance reviews the candidate finding, looking for logical inconsistencies or signs of hallucination.
3. **Deduplication**: stack-trace clustering removes variants of the same underlying bug before human review.
4. **Human triage**: security researchers validate the remaining findings and write or review patches before external disclosure.

```python
def validate_crash(agent_output: CrashReport, binary: Path) -> ValidationResult:
    # Step 1: deterministic reproduction
    result = run_with_sanitizers(binary, agent_output.input_bytes)
    if not result.crashed:
        return ValidationResult(valid=False, reason="no_repro")

    # Step 2: LLM critique of the crash trace
    critique = llm_critique(
        crash_trace=result.trace,
        agent_reasoning=agent_output.reasoning,
    )
    if critique.likely_hallucination:
        return ValidationResult(valid=False, reason="hallucination")

    # Step 3: deduplication by normalized stack hash
    stack_hash = normalize_stack(result.trace)
    if stack_hash in seen_hashes:
        return ValidationResult(valid=False, reason="duplicate")

    seen_hashes.add(stack_hash)
    return ValidationResult(valid=True, crash=result)
```

Focusing initially on memory-corruption bugs (buffer overflows, use-after-free, integer overflows leading to out-of-bounds writes) is a deliberate engineering choice: these classes produce observable, unambiguous signals (process crashes, sanitizer reports) that make automated validation tractable. Logic errors that leave the program functional are harder to validate programmatically and are better reserved for later pipeline stages.

## Sandboxing and Misuse Constraints

An agent capable of discovering real vulnerabilities in arbitrary code is, by construction, also capable of being redirected toward offensive use. The engineering controls that matter most are:

- **Network isolation**: the sandbox has no outbound network access, preventing the agent from exfiltrating findings, fetching external payloads, or interacting with live targets.
- **Filesystem scope limits**: the agent can only read and write within a defined working directory; it cannot access credentials, SSH keys, or other sensitive host data.
- **Output filtering**: findings leaving the sandbox pass through a policy layer that checks for exploit code or weaponized proof-of-concept payloads before the data reaches human reviewers.
- **Rate and scope controls**: the orchestration layer restricts which codebases can be targeted, preventing the pipeline from being pointed at proprietary or out-of-scope targets.

<Callout type="warning">
Sandbox escapes are a real threat model. The agent environment should be treated as potentially hostile: use hardware-level isolation (VMs, not just containers) and assume the agent may attempt to circumvent restrictions if given sufficient reasoning capability.
</Callout>

## Scaling the Pipeline

Single-agent, sequential analysis is too slow for large codebases. Practical deployments parallelize across multiple agent instances, each assigned a module or subsystem. An orchestration layer handles task dispatch, aggregates crash reports, runs the deduplication step across all instances, and queues validated findings for human review.

Patch generation is the next bottleneck after discovery. As finding volume grows, human patch writing becomes the rate-limiting step. Current research directions include using the same agent to draft patches immediately after validating a bug, then routing the draft through human review for correctness and completeness. This shifts human effort from writing patches from scratch to reviewing agent-generated ones—a substantially faster workflow at scale, though it requires careful review processes to avoid introducing new bugs through automated fixes.

<Callout type="tip">
Open-source projects are a practical starting target: source is available, the build environment is reproducible, and responsible disclosure processes are established. The agent pipeline can be validated on known CVEs with available PoCs before pointing it at unpatched code.
</Callout>

LLM-driven vulnerability discovery represents a meaningful shift in what automated security tooling can accomplish. The key engineering insight is that LLMs and traditional dynamic analysis tools are complements, not substitutes—the most effective pipelines use LLM reasoning to direct fuzzing and static analysis rather than replace them, then invest heavily in validation infrastructure to keep false-positive rates low enough to be useful to human maintainers.
