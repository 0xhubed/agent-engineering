---
import BaseLayout from '../../../layouts/BaseLayout.astro';
import CodeBlock from '../../../components/CodeBlock.astro';
import Callout from '../../../components/Callout.astro';
import Table from '../../../components/Table.astro';
import Diagram from '../../../components/Diagram.astro';

const basicToolExample = [
  {
    language: 'pseudo',
    label: 'Pseudo-code',
    code: `function executeWithTools(userRequest, tools):
    # Format tools for the model
    toolDefinitions = formatToolDefinitions(tools)

    # Send request with tool definitions
    response = llm.generate(
        messages: [userRequest],
        tools: toolDefinitions
    )

    # Check if model wants to call a tool
    if response.hasToolCall:
        toolName = response.toolCall.name
        toolArgs = response.toolCall.arguments

        # Execute the tool
        result = tools[toolName].execute(toolArgs)

        # Send result back to model for final response
        return llm.generate(
            messages: [userRequest, response, toolResult(result)]
        )

    return response.content`,
  },
  {
    language: 'python',
    label: 'Python (LangChain)',
    code: `from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

# Define tools using the @tool decorator
@tool
def get_weather(location: str, unit: str = "celsius") -> str:
    """Get current weather for a location.

    Args:
        location: City name, e.g. 'San Francisco'
        unit: Temperature unit (celsius or fahrenheit)
    """
    # Implementation specific
    return weather_api.get_current(location, unit)

# Create LLM with tools bound
llm = ChatOpenAI(model="gpt-4")
llm_with_tools = llm.bind_tools([get_weather])

def execute_with_tools(user_message: str) -> str:
    messages = [HumanMessage(content=user_message)]

    # Get response (may include tool calls)
    response = llm_with_tools.invoke(messages)

    if response.tool_calls:
        # Execute each tool call
        messages.append(response)

        for tool_call in response.tool_calls:
            # LangChain routes to correct tool automatically
            result = get_weather.invoke(tool_call["args"])
            messages.append(
                ToolMessage(content=result, tool_call_id=tool_call["id"])
            )

        # Get final response with tool results
        return llm_with_tools.invoke(messages).content

    return response.content`,
  },
  {
    language: 'csharp',
    label: 'C# (Agent Framework)',
    code: `using Microsoft.Agents.AI;
using Microsoft.Extensions.AI;
using System.ComponentModel;
using Azure.AI.OpenAI;
using Azure.Identity;

// Define tool as a function with descriptions
[Description("Get current weather for a location")]
static string GetWeather(
    [Description("City name")] string location,
    [Description("Temperature unit")] string unit = "celsius")
{
    return weatherService.GetCurrent(location, unit);
}

// Create agent with tools
AIAgent agent = new AzureOpenAIClient(
    new Uri("https://your-resource.openai.azure.com"),
    new AzureCliCredential())
    .GetChatClient("gpt-4o")
    .AsAIAgent(
        instructions: "You are a helpful weather assistant",
        tools: [AIFunctionFactory.Create(GetWeather)]
    );

// Run the agent - it automatically calls tools as needed
var result = await agent.RunAsync("What's the weather in Tokyo?");

Console.WriteLine(result);`,
  },
];

const parallelToolExample = [
  {
    language: 'pseudo',
    label: 'Pseudo-code',
    code: `function executeParallelTools(userRequest, tools):
    response = llm.generate(
        messages: [userRequest],
        tools: tools,
        parallelToolCalls: true
    )

    if response.hasMultipleToolCalls:
        # Execute all tool calls concurrently
        results = parallel.map(response.toolCalls, (call) =>
            tools[call.name].execute(call.arguments)
        )

        # Collect all results
        toolResults = zip(response.toolCalls, results)

        return llm.generate(
            messages: [userRequest, response, ...toolResults]
        )

    return response.content`,
  },
  {
    language: 'python',
    label: 'Python (LangChain)',
    code: `import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

@tool
async def get_weather(location: str) -> str:
    """Get current weather for a location."""
    return await weather_api.get_async(location)

@tool
async def search_web(query: str) -> str:
    """Search the web for information."""
    return await search_api.search_async(query)

@tool
async def get_stock_price(symbol: str) -> str:
    """Get current stock price."""
    return await stock_api.get_price_async(symbol)

# Bind multiple tools
llm = ChatOpenAI(model="gpt-4")
tools = [get_weather, search_web, get_stock_price]
llm_with_tools = llm.bind_tools(tools)

async def agent_with_parallel_tools(query: str) -> str:
    messages = [HumanMessage(content=query)]

    response = await llm_with_tools.ainvoke(messages)

    if response.tool_calls:
        messages.append(response)

        # Execute all tools in parallel
        tool_tasks = []
        for tool_call in response.tool_calls:
            tool_fn = next(t for t in tools if t.name == tool_call["name"])
            tool_tasks.append(tool_fn.ainvoke(tool_call["args"]))

        results = await asyncio.gather(*tool_tasks)

        # Add all results
        for tool_call, result in zip(response.tool_calls, results):
            messages.append(
                ToolMessage(content=result, tool_call_id=tool_call["id"])
            )

        return (await llm_with_tools.ainvoke(messages)).content

    return response.content`,
  },
  {
    language: 'csharp',
    label: 'C# (Agent Framework)',
    code: `using Microsoft.Agents.AI;
using Microsoft.Extensions.AI;
using System.ComponentModel;

// Define multiple tools as functions
[Description("Get weather for a location")]
static async Task<string> GetWeather(string location)
    => await weatherService.GetAsync(location);

[Description("Search the web")]
static async Task<string> SearchWeb(string query)
    => await searchService.SearchAsync(query);

[Description("Get stock price")]
static async Task<string> GetStockPrice(string symbol)
    => await stockService.GetPriceAsync(symbol);

// Create agent with multiple tools
AIAgent agent = new AzureOpenAIClient(endpoint, credentials)
    .GetChatClient("gpt-4o")
    .AsAIAgent(
        instructions: "You are a helpful assistant",
        tools: [
            AIFunctionFactory.Create(GetWeather),
            AIFunctionFactory.Create(SearchWeb),
            AIFunctionFactory.Create(GetStockPrice)
        ]
    );

// Agent automatically handles parallel tool execution
var result = await agent.RunAsync(
    "What's the weather in NYC and Tokyo, and Apple's stock price?"
);`,
  },
];

const errorHandlingExample = [
  {
    language: 'pseudo',
    label: 'Pseudo-code',
    code: `function executeWithRetry(toolCall, maxRetries = 3):
    for attempt in range(maxRetries):
        try:
            result = tools[toolCall.name].execute(toolCall.args)
            return { success: true, data: result }
        catch error:
            if isRetryable(error) and attempt < maxRetries - 1:
                wait(exponentialBackoff(attempt))
                continue
            else:
                return {
                    success: false,
                    error: formatError(error)
                }

function agentLoop(userRequest):
    while not complete:
        response = llm.generate(messages, tools)

        if response.hasToolCall:
            result = executeWithRetry(response.toolCall)

            if not result.success:
                # Let the model know about the failure
                messages.append(toolError(result.error))
                # Model can try a different approach
            else:
                messages.append(toolResult(result.data))
        else:
            return response.content`,
  },
  {
    language: 'python',
    label: 'Python (LangChain)',
    code: `from langchain_core.tools import tool, ToolException
from langchain_core.runnables import RunnableConfig
from tenacity import retry, stop_after_attempt, wait_exponential

# Tools can handle their own errors gracefully
@tool(handle_tool_error=True)
def search_database(query: str) -> str:
    """Search the database with automatic error handling."""
    try:
        return db.search(query)
    except DatabaseError as e:
        # Return error message instead of raising
        raise ToolException(f"Database error: {e}")

# Custom retry wrapper for tools
class RetryableTool:
    def __init__(self, tool_fn, max_retries=3):
        self.tool = tool_fn
        self.max_retries = max_retries

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    def invoke(self, args: dict) -> str:
        return self.tool.invoke(args)

    def safe_invoke(self, args: dict) -> dict:
        try:
            result = self.invoke(args)
            return {"success": True, "data": result}
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }

# Using LangGraph for agent with error recovery
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    llm,
    tools,
    # Agent can see tool errors and try alternatives
    handle_tool_errors=True
)

# The agent will receive error messages and can adapt
result = agent.invoke({"messages": [("user", query)]})`,
  },
  {
    language: 'csharp',
    label: 'C# (Agent Framework)',
    code: `using Microsoft.Agents.AI;
using Microsoft.Extensions.AI;
using System.ComponentModel;
using Polly;

// Create retry policy
var retryPolicy = Policy
    .Handle<HttpRequestException>()
    .Or<TimeoutException>()
    .WaitAndRetryAsync(
        3,
        attempt => TimeSpan.FromSeconds(Math.Pow(2, attempt)),
        onRetry: (ex, time, attempt, ctx) =>
            Console.WriteLine($"Retry {attempt}: {ex.Message}")
    );

// Define tool with built-in error handling
[Description("Search with automatic retry")]
async Task<string> SearchWithRetry(string query)
{
    try
    {
        return await retryPolicy.ExecuteAsync(async () =>
        {
            return await searchService.SearchAsync(query);
        });
    }
    catch (Exception ex)
    {
        // Return error info so agent can adapt
        return $"Error: {ex.Message}. Try a different approach.";
    }
}

// Create agent with resilient tools
AIAgent agent = new AzureOpenAIClient(endpoint, credentials)
    .GetChatClient("gpt-4o")
    .AsAIAgent(
        instructions: "You are a helpful assistant",
        tools: [AIFunctionFactory.Create(SearchWithRetry)]
    );

// Agent receives error messages and can adapt its approach
var result = await agent.RunAsync(prompt);`,
  },
];
---

<BaseLayout
  title="Tool Use & Function Calling"
  description="Learn how AI agents extend their capabilities by calling external tools and APIs"
>
  <article class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12">
    <!-- Header -->
    <header class="mb-12">
      <nav class="mb-4">
        <a
          href="/topics/"
          class="text-sm text-primary-600 dark:text-primary-400 hover:underline"
        >
          &larr; Back to Topics
        </a>
      </nav>
      <h1 class="text-3xl sm:text-4xl font-bold text-slate-900 dark:text-white mb-4">
        Tool Use & Function Calling
      </h1>
      <p class="text-lg text-slate-600 dark:text-slate-300">
        The bridge between language models and real-world actions. Tool use enables agents to interact with external systems, APIs, and databases.
      </p>
    </header>

    <!-- Concept Overview -->
    <section class="prose prose-slate dark:prose-invert max-w-none mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        What is Tool Use?
      </h2>
      <p class="text-slate-600 dark:text-slate-300 mb-4">
        Tool use (also called function calling) allows language models to request the execution of external functions. Instead of just generating text, the model can:
      </p>
      <ul class="list-disc list-inside text-slate-600 dark:text-slate-300 space-y-2 mb-6">
        <li>Query databases and APIs</li>
        <li>Perform calculations</li>
        <li>Read and write files</li>
        <li>Execute code</li>
        <li>Interact with external services</li>
      </ul>

      <Callout type="info" title="Key Insight">
        Tools transform LLMs from text generators into agents that can take action in the world. The model decides <em>when</em> to use a tool, <em>which</em> tool to use, and <em>what arguments</em> to pass.
      </Callout>
    </section>

    <!-- How It Works -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        How Tool Calling Works
      </h2>

      <Diagram title="Tool Calling Flow">
{`User Request
     │
     ▼
┌─────────────────────────────────────┐
│           Language Model            │
│   (with tool definitions loaded)    │
└─────────────────────────────────────┘
     │
     │ Model decides to call tool
     ▼
┌─────────────────────────────────────┐
│         Tool Call Response          │
│  { name: "get_weather",             │
│    arguments: { "location": "NYC" } │
│  }                                  │
└─────────────────────────────────────┘
     │
     │ Application executes tool
     ▼
┌─────────────────────────────────────┐
│          Tool Execution             │
│   get_weather("NYC") → result       │
└─────────────────────────────────────┘
     │
     │ Result sent back to model
     ▼
┌─────────────────────────────────────┐
│           Language Model            │
│    (generates final response)       │
└─────────────────────────────────────┘
     │
     ▼
Final Response to User`}
      </Diagram>

      <div class="mt-6">
        <h3 class="text-lg font-semibold text-slate-900 dark:text-white mb-3">
          Basic Tool Execution
        </h3>
        <CodeBlock tabs={basicToolExample} title="Simple Tool Calling Pattern" />
      </div>
    </section>

    <!-- Tool Definition Formats -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        Tool Definition Formats
      </h2>
      <p class="text-slate-600 dark:text-slate-300 mb-4">
        Different LLM providers use slightly different formats for defining tools:
      </p>

      <Table caption="Tool definition formats vary by provider but share common elements">
        <thead>
          <tr>
            <th>Provider</th>
            <th>Format</th>
            <th>Key Features</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OpenAI</td>
            <td>JSON Schema</td>
            <td>Parallel calls, strict mode, function descriptions</td>
          </tr>
          <tr>
            <td>Anthropic</td>
            <td>JSON Schema</td>
            <td>Tool use blocks, detailed descriptions encouraged</td>
          </tr>
          <tr>
            <td>Google</td>
            <td>OpenAPI-style</td>
            <td>Function declarations with protobuf types</td>
          </tr>
          <tr>
            <td>Open Models</td>
            <td>Varies</td>
            <td>Often use Hermes or ChatML format</td>
          </tr>
        </tbody>
      </Table>

      <Callout type="tip" title="Best Practice">
        Write detailed tool descriptions. Models use these descriptions to decide when to call a tool. Include examples of valid inputs and explain edge cases.
      </Callout>
    </section>

    <!-- Parallel Tool Calls -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        Parallel Tool Execution
      </h2>
      <p class="text-slate-600 dark:text-slate-300 mb-4">
        Modern LLMs can request multiple tool calls simultaneously, dramatically reducing latency for complex tasks:
      </p>

      <CodeBlock tabs={parallelToolExample} title="Executing Multiple Tools Concurrently" />

      <Callout type="warning" title="Consideration">
        Not all tool calls should be parallelized. If tools have dependencies (e.g., create record then update it), they must be executed sequentially.
      </Callout>
    </section>

    <!-- Error Handling -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        Error Handling & Retries
      </h2>
      <p class="text-slate-600 dark:text-slate-300 mb-4">
        Robust tool execution requires handling failures gracefully. The model should be informed of errors so it can try alternative approaches:
      </p>

      <CodeBlock tabs={errorHandlingExample} title="Resilient Tool Execution" />
    </section>

    <!-- Trade-offs -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        Trade-offs & Approaches
      </h2>

      <Table>
        <thead>
          <tr>
            <th>Approach</th>
            <th>Pros</th>
            <th>Cons</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Static tool list</strong></td>
            <td>Simple, predictable, easy to test</td>
            <td>Context bloat with many tools</td>
          </tr>
          <tr>
            <td><strong>Dynamic discovery</strong></td>
            <td>Scales to many tools</td>
            <td>Additional latency, complexity</td>
          </tr>
          <tr>
            <td><strong>Tool clustering</strong></td>
            <td>Balance of both approaches</td>
            <td>Routing logic complexity</td>
          </tr>
          <tr>
            <td><strong>Skills pattern</strong></td>
            <td>Massive token savings (98%+)</td>
            <td>Requires filesystem access</td>
          </tr>
        </tbody>
      </Table>

      <Callout type="info" title="Related Topic">
        For large tool libraries (50+ tools), see the <a href="/topics/skills-pattern/" class="text-primary-600 dark:text-primary-400 hover:underline">Skills Pattern</a> which can reduce context usage from 150K to 2K tokens.
      </Callout>
    </section>

    <!-- Evaluation -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        Evaluation Approach
      </h2>
      <p class="text-slate-600 dark:text-slate-300 mb-4">
        Measuring tool use quality requires evaluating multiple dimensions:
      </p>

      <Table caption="Key metrics for evaluating tool calling capabilities">
        <thead>
          <tr>
            <th>Metric</th>
            <th>What it Measures</th>
            <th>How to Calculate</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Tool Selection Accuracy</strong></td>
            <td>Did the model pick the right tool?</td>
            <td>Compare selected tool vs ground truth</td>
          </tr>
          <tr>
            <td><strong>Argument Accuracy</strong></td>
            <td>Were the parameters correct?</td>
            <td>Exact match or semantic similarity of args</td>
          </tr>
          <tr>
            <td><strong>Unnecessary Calls</strong></td>
            <td>Did model call tools when not needed?</td>
            <td>Count tool calls for simple queries</td>
          </tr>
          <tr>
            <td><strong>Latency Impact</strong></td>
            <td>Time added by tool execution</td>
            <td>End-to-end time minus model inference</td>
          </tr>
          <tr>
            <td><strong>Error Recovery</strong></td>
            <td>Can model handle tool failures?</td>
            <td>Success rate after simulated failures</td>
          </tr>
        </tbody>
      </Table>

      <div class="mt-6 p-4 bg-slate-50 dark:bg-slate-800 rounded-lg">
        <h3 class="font-semibold text-slate-900 dark:text-white mb-2">
          Evaluation Frameworks
        </h3>
        <ul class="list-disc list-inside text-slate-600 dark:text-slate-300 space-y-1">
          <li><strong>DeepEval</strong> - ToolCorrectnessMetric, ArgumentCorrectnessMetric</li>
          <li><strong>ToolBench</strong> - Large-scale tool use benchmark</li>
          <li><strong>API-Bank</strong> - API call evaluation dataset</li>
          <li><strong>Custom datasets</strong> - Domain-specific tool calling tests</li>
        </ul>
      </div>
    </section>

    <!-- Common Pitfalls -->
    <section class="mb-12">
      <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4">
        Common Pitfalls
      </h2>

      <div class="space-y-4">
        <Callout type="danger" title="Insufficient Tool Descriptions">
          Vague descriptions lead to incorrect tool selection. Always include examples and edge cases in your tool definitions.
        </Callout>

        <Callout type="danger" title="Missing Error Information">
          When a tool fails, send the error details back to the model. Without this, the model can't reason about what went wrong.
        </Callout>

        <Callout type="danger" title="Unbounded Tool Loops">
          Always set a maximum iteration limit. Without it, agents can get stuck in infinite tool-calling loops.
        </Callout>

        <Callout type="warning" title="Security: Unvalidated Arguments">
          Never pass tool arguments directly to system commands. Validate and sanitize all inputs to prevent injection attacks.
        </Callout>
      </div>
    </section>

    <!-- Next Steps -->
    <section class="border-t border-slate-200 dark:border-slate-700 pt-8">
      <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-4">
        Related Topics
      </h2>
      <div class="grid sm:grid-cols-2 gap-4">
        <a
          href="/topics/react-pattern/"
          class="block p-4 bg-slate-50 dark:bg-slate-800 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors"
        >
          <h3 class="font-semibold text-slate-900 dark:text-white mb-1">ReAct Pattern</h3>
          <p class="text-sm text-slate-600 dark:text-slate-400">
            Combine tool use with reasoning for more capable agents
          </p>
        </a>
        <a
          href="/topics/memory/"
          class="block p-4 bg-slate-50 dark:bg-slate-800 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors"
        >
          <h3 class="font-semibold text-slate-900 dark:text-white mb-1">Memory Systems</h3>
          <p class="text-sm text-slate-600 dark:text-slate-400">
            Store tool results for future reference and learning
          </p>
        </a>
      </div>
    </section>
  </article>
</BaseLayout>
