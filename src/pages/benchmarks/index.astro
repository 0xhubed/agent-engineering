---
import BaseLayout from '../../layouts/BaseLayout.astro';
import Callout from '../../components/Callout.astro';
import Table from '../../components/Table.astro';
import Diagram from '../../components/Diagram.astro';

const benchmarks = [
  {
    name: 'SWE-bench Verified',
    description: 'Gold standard for code agents. 500 human-verified GitHub issues from 12 Python repositories. Agent must generate patches that pass test suites.',
    category: 'Software Engineering',
    topScore: '72%',
    topAgent: 'Claude 3.5 Sonnet + Devin',
    difficulty: 'Hard',
    tasks: 500,
    link: 'https://www.swebench.com/'
  },
  {
    name: 'SWE-bench Full',
    description: 'Complete dataset of 2,294 real GitHub issues. More diverse but noisier than Verified subset.',
    category: 'Software Engineering',
    topScore: '51%',
    topAgent: 'Various',
    difficulty: 'Hard',
    tasks: 2294,
    link: 'https://www.swebench.com/'
  },
  {
    name: 'WebArena',
    description: 'Web navigation benchmark with 812 tasks across 5 self-hosted websites (shopping, Reddit, GitLab, OpenStreetMap, Wikipedia).',
    category: 'Web Navigation',
    topScore: '42%',
    topAgent: 'GPT-4V + SoM',
    difficulty: 'Hard',
    tasks: 812,
    link: 'https://webarena.dev/'
  },
  {
    name: 'GAIA',
    description: 'General AI Assistant benchmark. 466 questions requiring web search, file processing, and multi-step reasoning.',
    category: 'General Assistant',
    topScore: '75% (L1)',
    topAgent: 'Various',
    difficulty: 'Medium-Hard',
    tasks: 466,
    link: 'https://huggingface.co/gaia-benchmark'
  },
  {
    name: 'τ-bench (Tau-bench)',
    description: 'Multi-turn conversations requiring tool use across airline, retail, and banking domains with simulated APIs.',
    category: 'Tool + Conversation',
    topScore: '45% (Retail)',
    topAgent: 'GPT-4',
    difficulty: 'Medium',
    tasks: 680,
    link: 'https://github.com/sierra-research/tau-bench'
  },
  {
    name: 'AgentBench',
    description: 'Multi-dimensional evaluation across 8 environments: OS, DB, KG, Digital Cards, Lateral Thinking, House-Holding, Web Shopping, Web Browsing.',
    category: 'Multi-Environment',
    topScore: 'Model-dependent',
    topAgent: 'GPT-4',
    difficulty: 'Varied',
    tasks: 1632,
    link: 'https://github.com/THUDM/AgentBench'
  },
  {
    name: 'HumanEval',
    description: 'Code generation benchmark. 164 Python programming problems testing functional correctness.',
    category: 'Code Generation',
    topScore: '95%+',
    topAgent: 'Various',
    difficulty: 'Medium',
    tasks: 164,
    link: 'https://github.com/openai/human-eval'
  },
  {
    name: 'MBPP',
    description: 'Mostly Basic Python Problems. 974 entry-level programming tasks.',
    category: 'Code Generation',
    topScore: '85%+',
    topAgent: 'Various',
    difficulty: 'Easy-Medium',
    tasks: 974,
    link: 'https://github.com/google-research/google-research/tree/master/mbpp'
  },
];

const difficultyStyle = (d: string) => {
  if (d === 'Hard') return 'font-small-caps text-xs text-red-800 border border-red-300 px-1.5 py-0.5';
  if (d === 'Medium') return 'font-small-caps text-xs text-amber-800 border border-amber-300 px-1.5 py-0.5';
  return 'font-small-caps text-xs text-ink-muted border border-newsprint-300 px-1.5 py-0.5';
};

const categories = [...new Set(benchmarks.map(b => b.category))];
---

<BaseLayout title="Benchmarks">
  <div class="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8 py-12">

    <!-- Header -->
    <div class="text-center mb-8">
      <h1 class="font-display font-bold text-ink" style="font-size: clamp(1.75rem, 4vw, 3rem);">
        Agent Benchmarks
      </h1>
      <p class="font-serif italic text-ink-muted mt-2 max-w-2xl mx-auto">
        Standardized measures of AI agent capability — from software engineering to web navigation.
      </p>
      <hr class="rule-double mt-6" />
    </div>

    <!-- Landscape Diagram -->
    <section class="mb-10">
      <Diagram>
{`┌─────────────────────────────────────────────────────────────────────────────┐
│                        Agent Benchmark Landscape                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  CODE & SOFTWARE                    WEB & NAVIGATION                        │
│  ┌────────────────────┐             ┌────────────────────┐                  │
│  │ SWE-bench          │             │ WebArena           │                  │
│  │ HumanEval          │             │ MiniWoB++          │                  │
│  │ MBPP               │             │ Mind2Web           │                  │
│  └────────────────────┘             └────────────────────┘                  │
│                                                                             │
│  GENERAL ASSISTANT                  TOOL & FUNCTION                         │
│  ┌────────────────────┐             ┌────────────────────┐                  │
│  │ GAIA               │             │ τ-bench            │                  │
│  │ AgentBench         │             │ API-Bank           │                  │
│  │ AssistantBench     │             │ ToolBench          │                  │
│  └────────────────────┘             └────────────────────┘                  │
│                                                                             │
│  Human Baseline ████████████████████████████████████████████ 78-95%        │
│  Current SOTA   ██████████████████████                       40-72%        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘`}
      </Diagram>
    </section>

    <!-- State of the Art -->
    <section class="mb-10">
      <h2 class="font-display font-bold text-ink text-xl mb-4">Current State of the Art (2025)</h2>
      <Callout type="info" title="Human vs AI Gap">
        The gap between human performance and AI agent performance remains significant on most benchmarks, especially those requiring long-horizon planning, error recovery, and real-world interaction.
      </Callout>
      <div class="mt-6">
        <Table caption="Current state of the art vs human baseline">
          <thead>
            <tr>
              <th>Benchmark</th>
              <th>Top Score</th>
              <th>Leading System</th>
              <th>Human Baseline</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>SWE-bench Verified</strong></td>
              <td>72%</td>
              <td>Claude 3.5 + Devin</td>
              <td>~100%</td>
            </tr>
            <tr>
              <td><strong>GAIA (L1)</strong></td>
              <td>75%</td>
              <td>Various</td>
              <td>92%</td>
            </tr>
            <tr>
              <td><strong>WebArena</strong></td>
              <td>42%</td>
              <td>GPT-4V + SoM</td>
              <td>78%</td>
            </tr>
            <tr>
              <td><strong>HumanEval</strong></td>
              <td>95%+</td>
              <td>Various</td>
              <td>~100%</td>
            </tr>
          </tbody>
        </Table>
      </div>
    </section>

    <!-- Benchmark cards by category -->
    {categories.map(cat => (
      <section class="mb-10">
        <h2 class="section-label mb-4">{cat}</h2>
        <div class="space-y-0 divide-y divide-newsprint-200">
          {benchmarks.filter(b => b.category === cat).map(bm => (
            <div class="py-5">
              <div class="flex items-start justify-between gap-4 mb-2">
                <div>
                  <a href={bm.link} target="_blank" rel="noopener noreferrer"
                     class="font-display font-bold text-ink text-lg hover:underline no-underline">
                    {bm.name}
                  </a>
                  <span class={`ml-3 font-serif ${difficultyStyle(bm.difficulty)}`}>
                    {bm.difficulty}
                  </span>
                </div>
                <div class="text-right shrink-0">
                  <div class="font-display font-bold text-ink text-xl">{bm.topScore}</div>
                  <div class="font-serif text-xs text-ink-muted">top score</div>
                </div>
              </div>
              <p class="font-serif text-sm text-ink-muted leading-relaxed">{bm.description}</p>
              <div class="flex gap-4 mt-2 font-serif text-xs text-ink-faint">
                <span>{bm.tasks.toLocaleString()} tasks</span>
                <span>·</span>
                <span>Best: {bm.topAgent}</span>
              </div>
            </div>
          ))}
        </div>
      </section>
    ))}

    <!-- Methodology note -->
    <section class="mt-10 pt-6 border-t border-newsprint-300">
      <h2 class="font-display font-bold text-ink text-xl mb-4">Evaluation Methodology</h2>
      <p class="font-serif text-ink-light leading-relaxed mb-4">
        Rigorous benchmark evaluation requires deterministic execution (temperature=0), isolated environments, and statistical confidence intervals. A common error is reporting accuracy without confidence bounds — on small benchmarks like HumanEval (164 tasks), a 2-point difference may not be statistically significant.
      </p>
      <Callout type="tip" title="Wilson Score Intervals">
        Always report 95% confidence intervals alongside benchmark scores. The Wilson score interval is preferred over the normal approximation for proportions near 0 or 1.
      </Callout>
    </section>

  </div>
</BaseLayout>
